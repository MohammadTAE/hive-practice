1) Create two table in LFS: (a) emps1 (b) dept1

  hive> create table emp(id int, name string, sal int, gender string, dno int)
        > row format delimited
        > fields terminated by ',';
  hive> load data local inpath '../emps1' into table emp;
  hive> hive> select * from emp;
        OK
        101	aaa	1000	m	11
        102	bbb	2000	f	12
  hive> create table dept(dno int, dname string, city string)
    > row format delimited
    > fields terminated by ',';
  hive> load data local inpath '../dept1' INTO TABLE dept;

  hive> select * from dept;
      OK
      11	mrkt	hyd
      12	HR	delhi

  ----------------------------------------------------------------------
                       JOINING
  --------------------------------------------------------------------
  hive> select e.id, e.name, e.sal, e.gender, e.dno, d.dno, d.dname, d.city
    > from emp as e 
    > join dept as d on (e.dno = d.dno);

  hive> select e.id, e.name, e.sal, e.gender, e.dno, d.dno, d.dname, d.city
    > from emp as e 
    > left outer join dept as d on (e.dno = d.dno);

  hive> select e.id, e.name, e.sal, e.gender, e.dno, d.dno, d.dname, d.city
    > from emp as e 
    > right outer join dept as d on (e.dno = d.dno);

  hive> select e.id, e.name, e.sal, e.gender, e.dno, d.dno, d.dname, d.city
    > from emp as e 
    > full outer join dept as d on (e.dno = d.dno);
 
  hive> create table fullouter(id int, name string, sal int, gender string, dno int, dname string, city string); 
  hive> insert overwrite table fullouter select e.id, e.name, e.sal, e.gender, e.dno, d.dname, d.city
    > from emp as e
    > full outer join dept as d on (e.dno= d.dno);
  hive> select * from fullouter limit 3;
OK
105	eee	5000	m	11	mrkt	hyd
101	aaa	1000	m	11	mrkt	hyd
103	ccc	3000	m	12	HR	delhi
 

------------------------------------------------------------------------------------------------
                                        JSON
--------------------------------------------------------------------------------------------
Create jason file in the LFS: json1 

hive> create table jsonsamp1(line string);
hive> select * from jsonsamp1;
     OK
     {"name":"Blake","age":25,"gender":"m"}
     {"name":"Ramya","age":27,"gender":"f"}
hive> select get_json_object(line,"$.name"),
      get_json_object(line,"$.age"),get_json_object(line,"$.gender"),
      get_json_object(line,"$.city") from jsonsamp1 limit 2;
    OK
   Blake	25	m	NULL
   Ramya	27	f	NULL

hive> select x.* from jsonsamp1 
    > lateral view json_tuple(line, 'name', 'age', 'gender', 'city') x as n,a,g,c;
    OK
    Blake	25	m	NULL
    Ramya	27	f	NULL
hive> create table jasonsamp3(name string, age int, gender string, city string);
hive> insert overwrite table jasonsamp3 select x.* from jsonsamp1 
hive> lateral view json_tuple(line, 'name','age','gender', 'city') x as n,a,g,c;
hive> select * from  jasonsamp3 limit 2;
OK
Blake	25	m	NULL
Ramya	27	f	NULL


----------------------------------NESTED JSON------------------------------
Create a json file at the LFS:
hive> load data local inpath '.../json2' into table js_nested; 
hove> select * from js_nested limit 2;
OK
{"name":"Ravi","age":25,"wife":{"name":"banu","age":24},"city":"hyd"}
{"name":"Ajay","age":26,"wife":{"name":"kavitha","age":21},"city":"pune"}

hive> create table js_nested_1(name string, age int, wife string, city string);
OK
Time taken: 0.203 seconds
hive> insert overwrite table js_nested_1
    > select x.* from js_nested
    > lateral view json_tuple(line, 'name', 'age', 'wife', 'city') x as n, a, w, c;
hive> select * from js_nested_1;
OK
Ravi	25	{"name":"banu","age":24}	hyd
Ajay	26	{"name":"kavitha","age":21}	pune

hive> create table js_nested_2(name string, emp_age int, wname string, wage int, city string);
OK
Time taken: 0.252 seconds
hive> insert overwrite table js_nested_2
    > select name, age, get_json_object(wife,'$.name'), get_json_object(wife, '$.age'), city from js_nested_1;
hive> select * from js_nested_2;
OK
Ravi	25	banu	24	hyd
Ajay	26	kavitha	21	pune
 
---------------------------------------------------URL----------------------------------------------
Create a url table in the LFS.

hive> create table url(line string); 
hive> load data local inpath '.../urls' into url;
select * from url;
http://yoursite.com/bigdata/mongodb?name=Raj&age=25&city=hyd
http://yoursite.com/bigdata/spark?name=Swapna&city=del&desig=se

hive> create table url_struct(host string, path string, query string);
hive> insert overwrite table url_struct
    > select x.* from url
    > lateral view parse_url_tuple(line, 'HOST', 'PATH', 'QUERY') x as h, p, q;
hive> select * from url_struct;
yoursite.com	/bigdata/mongodb	name=Raj&age=25&city=hyd
yoursite.com	/bigdata/spark	name=Swapna&city=del&desig=se

hive> create table url_struct1(host string, path array<string>, query map<string, string>);
hive> insert overwrite table url_struct1
    > select host, split(path, '/'), str_to_map(query, '&', '=') from url_struct;
hive> select * from url_struct1;
yoursite.com	["","bigdata","mongodb"]	{"name":"Raj","age":"25","city":"hyd"}
yoursite.com	["","bigdata","spark"]	{"name":"Swapna","city":"del","desig":"se"}

hive> select map_keys(query) from url_struct1; 
["name","age","city"]
["name","city","desig"]

hive> select map_values(query) from url_struct1;
["Raj","25","hyd"]
["Swapna","del","se"]

hive> create table url_struct2(host string, category string, course string, name string, age int, gender string, city string, desig string);
hive> insert overwrite table url_struct2
    > select host, path[1], path[2], query['name'], query['age'], query['gender'], query['city'], query['desig'] from url_struct1;
hive> select * from url_struct2;
yoursite.com	bigdata	mongodb	Raj	25	NULL	hyd	NULL
yoursite.com	bigdata	spark	Swapna	NULL	NULL	del	se

----------------------------------------------------------------XML------------------------
create table in LFS populated with xml file format data.

hive> create table xml(lin string);
hive> load data local inpath '../sml' into table xml;
hive> select * from xml;
<rec><name>Ravi</name><age>25</age><city>hyd</city></rec>
<rec><name>Rani</name><age>24</age><gender>f</gender></rec>
<rec><name>Sampath</name><gender>m</gender><city>Del</city></rec>
hive> select count(*) from xml;
OK
3
Time taken: 41.139 seconds, Fetched: 1 row(s)

hive> select xpath_string(lin, 'rec/name'), xpath_int(lin, 'rec/age'), xpath_string(lin, 'rec/gender'),xpath_string(lin, 'rec/city') from xml;
Ravi	25		hyd
Rani	24	f	
Sampath	0	m	Del

-----------------------------------------XML with NESTED TAGS--------------------------------------------------
Create a xml file in the LFS.

hive> load data local inpath '../xml2' into table xml_nest;
hive> select * from xml_nest;
<rec><name><fname>Ravi</fname><lname>kumar</lname></name><age>25</age><contact><email><personal>ravi@gmail.com</personal><official>ravi@infy.com</official></email><phone><mobile>9988665544</mobile><office>9977665544</office><residence>9955443333</residence></phone></contact></rec>

hive> select xpath_string(line,'rec/name/fname'),xpath_string(line,'rec/name/lname'),xpath_int(line,'rec/age'),xpath_string(line,'rec/contact/email/personal'),xpath_string(line,'rec/contact/email/official'),xpath_string(line,'rec/contact/phone/mobile'),xpath_string(line,'rec/contact/phone/office'),xpath_string(line,'rec/contact/phone/residence') from xml_nest;
OK
Ravi	kumar	25	ravi@gmail.com	ravi@infy.com	9988665544	9977665544	9955443333

---------------------------------------------------PARTITIONING --------------------------------

hive> create table emp_partition(id int, name string, sal int, gender string, dno int)
    > partitioned by (s string)
    > row format delimited 
    > fields terminated by ',';
OK
Time taken: 0.196 seconds
hive> insert overwrite table emp_partition
    > partition(s='m')
    > select * from emp where gender = 'm';
hive> select * from emp_partition limit 3;
OK
101	aaa	1000	m	11	m
103	ccc	3000	m	12	m
105	eee	5000	m	11	m

--------------------------------------------------BUCKETING -------------------------------

create a table called junesales in the LFS.
hive> load data local inpath '.../practice/junesale' into table junesales;
Loading data to table hive_queries2_practice.junesales
OK
Time taken: 0.648 seconds
hive> select * from junesales limit 5;
OK
p1	1000
p2	3000
p3	5000
p4	7000
p5	9000

hive> set hive.enforce.bucketing=true;
hive> create table junesales_bucket(pid string, price int)
    > clustered by (pid)
    > into 3 buckets;
OK
hive> insert overwrite table junesales_bucket 
    > select * from junesales;
hive> select * from junesales_bucket limit 4;
OK
p5	9000
	NULL
p2	3000
p2	3000


-------------------------------------------------Additional information for future----------------------
Integrating Sqoop with Hive

sqoop import --connect jdbc:mysql://localhost/testdb
--username root -P
--table table_name --hive-import --hive-table hive_table_name
